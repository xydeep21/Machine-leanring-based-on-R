---
title: "Boosting Tree, Bagging Tree & Rndom forests"
author: "YUANXU"
date: "10/27/2020"
output: html_document
---
## Rabdom forests and Bagging model using Boston data
```{r}
## Load packages and the Boston data
library(MASS)
library(randomForest)
attach(Boston)
set.seed(271)
Boston  = na.omit(Boston)
subset = sample(nrow(Boston),nrow(Boston)/3)
train_set = Boston[-subset,]
test_set = Boston[subset,]
dim(test_set)
dim(train_set)
train = Boston[-subset,"medv"]
```
```{r}
## mtry=13 indicates adding all 13 features in the model
## importance = "True" indicates calculating the importance score
bag_boston = randomForest(medv~., data=train_set, mtry=13, importance=TRUE)
pred_bag = predict(bag_boston,test_set)
plot(pred_bag,train_set$medv)
mean((pred_bag-train_set$medv)^2)
```
```{r}
## mtry = 6 means each time only need 6 features to build Randomforest models
rf_boston = randomForest(medv~., data = train_set , mtry =6 ,importance = TRUE)
pred_rf = predict(rf_boston,test_set)
plot(pred_rf,train_set$medv)
mean((pred_rf-train_set$medv)^2)
```
## Using changing parameters for learning rate to find the best parameter for building Boost Trees
```{r}
## Load the gbm package and usgin the same boston data
library(gbm)
## Build lambda which is a list contains the learning rate parameters
lambda = rep(NA,10)
for (i in 1:10){
  lambda[i]=0.01*i
}
```
```{r}
## Build the Boost Tree Model
set.seed(271)
gbm_pred = c()
for (i in 1:length(lambda)){
  gbm_boston = gbm(medv~.,data=train_set,distribution = "gaussian",shrinkage = lambda[i],n.trees = 5000,interaction.depth = 4)
  gbm_pred[i] = predict(gbm_boston, test_set,n.trees = 5000ï¼‰
}
gbm_pred
```
```{r}
set.seed(271)
gbm_error = c()
for (i in 1:length(gbm_pred)){
  gbm_error[i]=mean((gbm_pred[i]-test_set$medv)^2)
}
best_error = min(gbm_error)
best_error
```
